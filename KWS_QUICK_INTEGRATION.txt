# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘  QUICK START: Add These Lines to Your hello_ai.py                          â•‘
# â•‘  File: KWS_QUICK_INTEGRATION.txt                                            â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# STEP 1: Add imports at the TOP of hello_ai.py
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ADD THESE THREE LINES after existing imports:

from keyword_spotter import MultiKeywordSpotter
import numpy as np
import threading


# STEP 2: Create setup function BEFORE main loop
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ADD THIS FUNCTION before the audio processing loop:

def initialize_keyword_spotters():
    """Initialize KWS models for WHERE and SPEED keywords"""
    kws = MultiKeywordSpotter()
    try:
        kws.add_spotter('where', 'models/kws/where.tflite', threshold=0.85)
    except:
        print("âš ï¸  WHERE model not found at models/kws/where.tflite")
    
    try:
        kws.add_spotter('speed', 'models/kws/speed.tflite', threshold=0.85)
    except:
        print("âš ï¸  SPEED model not found at models/kws/speed.tflite")
    
    return kws


# STEP 3: Modify the main audio stream loop
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CHANGE THIS:
#
#  with sd.InputStream(device=MIC_INDEX, samplerate=RATE, channels=1, blocksize=4096):
#      recognizer = KaldiRecognizer(model, RATE)
#      recognizer.SetWords(wlist)
#      print("Listening...")
#      while True:
#          data = q.get()
#          if recognizer.AcceptWaveform(data):
#              result = json.loads(recognizer.Result())


# TO THIS:
#
#  with sd.InputStream(device=MIC_INDEX, samplerate=RATE, channels=1, blocksize=4096):
#      recognizer = KaldiRecognizer(model, RATE)
#      recognizer.SetWords(wlist)
#      
#      # â† ADD THIS SECTION â†
#      kws_detector = initialize_keyword_spotters()
#      print("ğŸ¤ Listening for 'WHERE' and 'SPEED'...")
#      # â† END NEW SECTION â†
#      
#      while True:
#          data = q.get()
#          
#          # â† ADD THIS SECTION â†
#          # Convert audio for KWS
#          try:
#              audio_array = np.frombuffer(data, dtype=np.int16).astype(np.float32) / 32768.0
#              detected = kws_detector.detect_any(audio_array)
#              if not detected and kws_detector.spotters:
#                  continue  # Skip Vosk if keyword not detected (optional optimization)
#          except Exception as e:
#              print(f"KWS error: {e}")
#          # â† END NEW SECTION â†
#          
#          if recognizer.AcceptWaveform(data):
#              result = json.loads(recognizer.Result())


# STEP 4: Prepare your model files
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Run these terminal commands:
#
#   # Create KWS directory
#   mkdir -p /Users/tusharbhaliya/Desktop/AI/respeaker-ai/models/kws
#
#   # Option A: Clone KWS-DS-CNN repo
#   git clone https://github.com/PeterMS123/KWS-DS-CNN-for-embedded.git /tmp/kws
#   
#   # Option B: Download pre-trained models (if available)
#   # curl -o models/kws/where.tflite [model-url]
#   # curl -o models/kws/speed.tflite [model-url]


# STEP 5: Install dependencies
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Terminal command:
#   pip install tensorflow numpy
# OR lightweight version for Raspberry Pi:
#   pip install tflite-runtime numpy


# STEP 6: Test it!
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Terminal command:
#   python hello_ai.py
#
# Then say "WHERE" or "SPEED" near the microphone
# You should see: "ğŸ¤ WHERE detected!" or similar


# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘  COMPLETE MINIMAL EXAMPLE                                                  â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

MINIMAL_EXAMPLE = """
# This is a complete minimal example showing full integration

import json, queue
import sounddevice as sd
from vosk import Model, KaldiRecognizer
import pyttsx3
import numpy as np
import google.generativeai as genai
from keyword_spotter import MultiKeywordSpotter

# Configuration
MIC_INDEX = 0  # Change this to your mic
RATE = 16000
MODEL_DIR = "models/en"
GEMINI_API_KEY = 'YOUR_API_KEY'

# Queue for audio data
q = queue.Queue()

def audio_callback(indata, frames, time, status):
    q.put(bytes(indata))

def initialize_keyword_spotters():
    kws = MultiKeywordSpotter()
    try:
        kws.add_spotter('where', 'models/kws/where.tflite', threshold=0.85)
        kws.add_spotter('speed', 'models/kws/speed.tflite', threshold=0.85)
    except Exception as e:
        print(f"âš ï¸  Could not load KWS models: {e}")
    return kws

def main():
    # Initialize Vosk
    model = Model(MODEL_DIR)
    recognizer = KaldiRecognizer(model, RATE)
    
    # Initialize KWS
    kws_detector = initialize_keyword_spotters()
    
    # Initialize Gemini
    genai.configure(api_key=GEMINI_API_KEY)
    ai_model = genai.GenerativeModel('gemini-pro')
    
    # Initialize TTS
    tts_engine = pyttsx3.init()
    
    print("ğŸ¤ Listening for 'WHERE' and 'SPEED'...")
    
    with sd.InputStream(device=MIC_INDEX, samplerate=RATE, channels=1, 
                       blocksize=4096, callback=audio_callback):
        while True:
            data = q.get()
            
            # Keyword spotting
            audio_array = np.frombuffer(data, dtype=np.int16).astype(np.float32) / 32768.0
            detected_keyword = kws_detector.detect_any(audio_array)
            
            if not detected_keyword:
                continue  # Skip Vosk if no keyword (optional)
            
            # Vosk speech recognition
            if recognizer.AcceptWaveform(data):
                result = json.loads(recognizer.Result())
                text_list = result.get("result", [])
                
                if text_list:
                    user_text = ' '.join(text_list)
                    print(f"ğŸ—£ï¸  You said: {user_text}")
                    
                    # Gemini AI
                    response = ai_model.generate_content(user_text)
                    ai_text = response.text
                    print(f"ğŸ¤– AI says: {ai_text}")
                    
                    # Text-to-Speech
                    tts_engine.say(ai_text)
                    tts_engine.runAndWait()

if __name__ == "__main__":
    main()
"""


# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘  DIRECTORY STRUCTURE AFTER INTEGRATION                                    â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

EXPECTED_STRUCTURE = """
respeaker-ai/
â”œâ”€â”€ hello_ai.py                        â† MODIFIED (add KWS)
â”œâ”€â”€ keyword_spotter.py                 â† NEW (provided)
â”œâ”€â”€ KWS_INTEGRATION_ANALYSIS.md        â† NEW (reference)
â”œâ”€â”€ KWS_INTEGRATION_GUIDE.py           â† NEW (reference)
â”œâ”€â”€ KWS_QUICK_INTEGRATION.txt          â† NEW (this file)
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ en/                            â† EXISTING (Vosk models)
â”‚   â”‚   â”œâ”€â”€ README
â”‚   â”‚   â”œâ”€â”€ am/
â”‚   â”‚   â”œâ”€â”€ conf/
â”‚   â”‚   â”œâ”€â”€ graph/
â”‚   â”‚   â””â”€â”€ ivector/
â”‚   â””â”€â”€ kws/                           â† NEW (Keyword spotting models)
â”‚       â”œâ”€â”€ where.tflite               â† ADD (train or download)
â”‚       â””â”€â”€ speed.tflite               â† ADD (train or download)
â”œâ”€â”€ requirements_rpi.txt               â† MODIFY (add tensorflow/tflite-runtime)
â””â”€â”€ ... other files ...
"""


# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘  SIMPLE COMPARISON: BEFORE vs AFTER                                       â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

COMPARISON = """
BEFORE:
  Vosk always listening â†’ Full speech recognition every time
  â”œâ”€ Fast response if speaking: ~500ms
  â”œâ”€ High CPU usage
  â””â”€ Many false positives

AFTER:
  KWS keyword detection â†’ Only run Vosk if keyword detected
  â”œâ”€ Very fast if keyword detected: ~100-150ms
  â”œâ”€ Low CPU usage (KWS is lightweight)
  â”œâ”€ Fewer false positives (only process relevant audio)
  â”œâ”€ Perfect for "WHERE" and "SPEED" detection
  â””â”€ Ideal for always-listening applications on Raspberry Pi
"""


# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘  FINAL CHECKLIST                                                          â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

CHECKLIST = """
Quick Integration Checklist:

1. Copy Files:
   â˜ Copy keyword_spotter.py to your project root
   
2. Add to hello_ai.py:
   â˜ Import: from keyword_spotter import MultiKeywordSpotter
   â˜ Import: import numpy as np
   â˜ Function: initialize_keyword_spotters()
   â˜ Init in loop: kws_detector = initialize_keyword_spotters()
   â˜ Detection: audio_array = np.frombuffer(...)
   â˜ Check: detected = kws_detector.detect_any(audio_array)
   â˜ Skip (optional): if not detected: continue
   
3. Get Models:
   â˜ Create: mkdir -p models/kws
   â˜ Add: where.tflite
   â˜ Add: speed.tflite
   
4. Install:
   â˜ pip install tensorflow numpy
   
5. Test:
   â˜ python hello_ai.py
   â˜ Say "WHERE"
   â˜ Say "SPEED"
   
6. Optimize:
   â˜ Adjust thresholds (0.85 is starting point)
   â˜ Test on Raspberry Pi
   â˜ Measure performance

Done! ğŸ‰
"""


if __name__ == "__main__":
    print(MINIMAL_EXAMPLE)
    print("\n" + "="*80 + "\n")
    print(EXPECTED_STRUCTURE)
    print("\n" + "="*80 + "\n")
    print(COMPARISON)
    print("\n" + "="*80 + "\n")
    print(CHECKLIST)
